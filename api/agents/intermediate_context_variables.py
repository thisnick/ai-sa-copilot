from .types import (
    ArtifactWithLinks,
    ArtifactSummary,
    ContextVariables,
    ResearchTopic,
    RunbookSection,
)


AFTER_REQUIREMENTS_GATHERING: ContextVariables = {
    "user_requirements": [
        "Deploy Databricks on AWS with a basic setup, without specific additional requirements."
    ],
    "research_topics": [
        ResearchTopic(
            research_question="How to deploy a basic setup of Databricks on AWS?",
            related_key_concepts="Integration with AWS services, Basic setup and configuration of Databricks, Data processing capabilities, Security and governance, Performance optimization, Cost management, Data Management and Collaboration in Cloud Environments.",
            related_user_requirements="Deploy Databricks on AWS with a basic setup, without specific additional requirements.",
        )
    ],
    "current_research_topic": 0,
}

AFTER_TOPIC_RESEARCH_COMPLETE: ContextVariables = {
    **AFTER_REQUIREMENTS_GATHERING,
    "saved_artifacts": {
        "How to deploy a basic setup of Databricks on AWS?": [
            ArtifactWithLinks(
                artifact_id="3e9c6f14-cc62-4dc2-aeeb-bbb8483964eb",
                url="https://docs.databricks.com/en/introduction/index.html",
                title="What is Databricks? | Databricks on AWS",
                summary="Databricks is a unified, open analytics platform designed for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale. It integrates seamlessly with cloud storage and security, managing infrastructure on behalf of users. This article explains how Databricks operates, its main features, and various use cases, making it a valuable resource for organizations looking to leverage data effectively.",
                parsed_text="* __[](https://www.databricks.com/)\n\n  * __[](https://www.databricks.com/)\n  * [Help Center](https://help.databricks.com/s/)\n  * [Documentation](https://docs.databricks.com/en/index.html)\n  * [Knowledge Base](https://kb.databricks.com/)\n\n  * [Community](https://community.databricks.com)\n  * [Support](https://help.databricks.com)\n  * [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n  * [Try Databricks](https://databricks.com/try-databricks)\n\n[English](javascript:void\\(0\\))\n\n  * [日本語](../../ja/introduction/index.html)\n  * [Português](../../pt/introduction/index.html)\n\n[Amazon Web Services](javascript:void\\(0\\))\n\n  * [Microsoft Azure](https://learn.microsoft.com/en/azure/databricks/introduction/)\n  * [Google Cloud Platform](https://docs.gcp.databricks.com/en/introduction/index.html)\n\n[Databricks on AWS](../index.html)\n\nGet started\n\n  * [Get started](../getting-started/index.html)\n  * What is Databricks?\n    * [ What is a data lakehouse?](../lakehouse/index.html)\n    * [ Apache Spark](../spark/index.html)\n    * [ What is Delta?](delta-comparison.html)\n    * [ Concepts](../getting-started/concepts.html)\n    * [ Architecture](../getting-started/overview.html)\n    * [ Integrations](../getting-started/connect/index.html)\n  * [DatabricksIQ](../databricksiq/index.html)\n  * [Release notes](../release-notes/index.html)\n\nLoad & manage data\n\n  * [Work with database objects](../database-objects/index.html)\n  * [Connect to data sources](../connect/index.html)\n  * [Connect to compute](../compute/index.html)\n  * [Discover data](../discover/index.html)\n  * [Query data](../query/index.html)\n  * [Ingest data](../ingestion/index.html)\n  * [Work with files](../files/index.html)\n  * [Transform data](../transform/index.html)\n  * [Schedule and orchestrate workflows](../jobs/index.html)\n  * [Monitor data and AI assets](../lakehouse-monitoring/index.html)\n  * [Share data securely](../data-sharing/index.html)\n\nWork with data\n\n  * [Data engineering](../data-engineering.html)\n  * [AI and machine learning](../machine-learning/index.html)\n  * [Generative AI tutorial](../generative-ai/tutorials/ai-cookbook/index.html)\n  * [Business intelligence](../ai-bi/index.html)\n  * [Data warehousing](../sql/index.html)\n  * [Notebooks](../notebooks/index.html)\n  * [Delta Lake](../delta/index.html)\n  * [Developers](../languages/index.html)\n  * [Technology partners](../integrations/index.html)\n\nAdministration\n\n  * [Account and workspace administration](../admin/index.html)\n  * [Security and compliance](../security/index.html)\n  * [Data governance (Unity Catalog)](../data-governance/index.html)\n  * [Lakehouse architecture](../lakehouse-architecture/index.html)\n\nReference & resources\n\n  * [Reference](../reference/api.html)\n  * [Resources](../resources/index.html)\n  * [What’s coming?](../whats-coming.html)\n  * [Documentation archive](../archive/index.html)\n\nUpdated Nov 14, 2024\n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation\nFeedback)\n\n  * [Documentation](../index.html)\n  * What is Databricks?\n  * \n\n# What is Databricks?\n\nSeptember 26, 2024\n\nDatabricks is a unified, open analytics platform for building, deploying,\nsharing, and maintaining enterprise-grade data, analytics, and AI solutions at\nscale. The Databricks Data Intelligence Platform integrates with cloud storage\nand security in your cloud account, and manages and deploys cloud\ninfrastructure on your behalf.\n\n**In this article:**\n\n  * How does a data intelligence platform work?\n  * What is Databricks used for?\n  * Managed integration with open source\n  * Tools and programmatic access\n  * How does Databricks work with AWS?\n  * What are common use cases for Databricks?\n  * Build an enterprise data lakehouse\n  * ETL and data engineering\n  * Machine learning, AI, and data science\n  * Data warehousing, analytics, and BI\n  * Data governance and secure data sharing\n  * DevOps, CI/CD, and task orchestration\n  * Real-time and streaming analytics\n\n## How does a data intelligence platform work?\n\nDatabricks uses generative AI with the [data\nlakehouse](../lakehouse/index.html) to understand the unique semantics of your\ndata. Then, it automatically optimizes performance and manages infrastructure\nto match your business needs.\n\nNatural language processing learns your business’s language, so you can search\nand discover data by asking a question in your own words. Natural language\nassistance helps you write code, troubleshoot errors, and find answers in\ndocumentation.\n\nFinally, your data and AI applications can rely on strong governance and\nsecurity. You can integrate APIs such as OpenAI without compromising data\nprivacy and IP control.\n\n## What is Databricks used for?\n\nDatabricks provides tools that help you connect your sources of data to one\nplatform to process, store, share, analyze, model, and monetize datasets with\nsolutions from BI to generative AI.\n\nThe Databricks workspace provides a unified interface and tools for most data\ntasks, including:\n\n  * Data processing scheduling and management, in particular ETL\n\n  * Generating dashboards and visualizations\n\n  * Managing security, governance, high availability, and disaster recovery\n\n  * Data discovery, annotation, and exploration\n\n  * Machine learning (ML) modeling, tracking, and model serving\n\n  * Generative AI solutions\n\n## Managed integration with open source\n\nDatabricks has a strong commitment to the open source community. Databricks\nmanages updates of open source integrations in the Databricks Runtime\nreleases. The following technologies are open source projects originally\ncreated by Databricks employees:\n\n  * [Delta Lake](https://delta.io/) and [Delta Sharing](https://delta.io/sharing)\n\n  * [MLflow](https://mlflow.org/)\n\n  * [Apache Spark](https://spark.apache.org/) and [Structured Streaming](https://spark.apache.org/streaming/)\n\n  * [Redash](https://redash.io/)\n\n## Tools and programmatic access\n\nDatabricks maintains a number of proprietary tools that integrate and expand\nthese technologies to add optimized performance and ease of use, such as the\nfollowing:\n\n  * [Jobs](../jobs/index.html)\n\n  * [Unity Catalog](../data-governance/unity-catalog/index.html)\n\n  * [Delta Live Tables](../delta-live-tables/index.html)\n\n  * [Databricks SQL](../sql/index.html)\n\n  * [Photon compute clusters](../compute/photon.html)\n\nIn addition to the workspace UI, you can interact with Databricks\nprogrammatically with the following tools:\n\n  * REST API\n\n  * CLI\n\n  * Terraform\n\n## How does Databricks work with AWS?\n\nThe Databricks platform architecture comprises two primary parts:\n\n  * The infrastructure used by Databricks to deploy, configure, and manage the platform and services.\n\n  * The customer-owned infrastructure managed in collaboration by Databricks and your company.\n\nUnlike many enterprise data companies, Databricks does not force you to\nmigrate your data into proprietary storage systems to use the platform.\nInstead, you configure a Databricks workspace by configuring secure\nintegrations between the Databricks platform and your cloud account, and then\nDatabricks deploys compute clusters using cloud resources in your account to\nprocess and store data in object storage and other integrated services you\ncontrol.\n\nUnity Catalog further extends this relationship, allowing you to manage\npermissions for accessing data using familiar SQL syntax from within\nDatabricks.\n\nDatabricks workspaces meet the security and networking requirements of [some\nof the world’s largest and most security-minded\ncompanies](https://www.databricks.com/customers). Databricks makes it easy for\nnew users to get started on the platform. It removes many of the burdens and\nconcerns of working with cloud infrastructure, without limiting the\ncustomizations and control experienced data, operations, and security teams\nrequire.\n\n## What are common use cases for Databricks?\n\nUse cases on Databricks are as varied as the data processed on the platform\nand the many personas of employees that work with data as a core part of their\njob. The following use cases highlight how users throughout your organization\ncan leverage Databricks to accomplish tasks essential to processing, storing,\nand analyzing the data that drives critical business functions and decisions.\n\n## Build an enterprise data lakehouse\n\nThe data lakehouse combines the strengths of enterprise data warehouses and\ndata lakes to accelerate, simplify, and unify enterprise data solutions. Data\nengineers, data scientists, analysts, and production systems can all use the\ndata lakehouse as their single source of truth, allowing timely access to\nconsistent data and reducing the complexities of building, maintaining, and\nsyncing many distributed data systems. See [What is a data\nlakehouse?](../lakehouse/index.html).\n\n## ETL and data engineering\n\nWhether you’re generating dashboards or powering artificial intelligence\napplications, data engineering provides the backbone for data-centric\ncompanies by making sure data is available, clean, and stored in data models\nthat allow for efficient discovery and use. Databricks combines the power of\nApache Spark with Delta Lake and custom tools to provide an unrivaled ETL\n(extract, transform, load) experience. You can use SQL, Python, and Scala to\ncompose ETL logic and then orchestrate scheduled job deployment with just a\nfew clicks.\n\n[Delta Live Tables](../delta-live-tables/index.html) simplifies ETL even\nfurther by intelligently managing dependencies between datasets and\nautomatically deploying and scaling production infrastructure to ensure timely\nand accurate delivery of data per your specifications.\n\nDatabricks provides a number of custom tools for [data\ningestion](../ingestion/index.html), including [Auto\nLoader](../ingestion/cloud-object-storage/auto-loader/index.html), an\nefficient and scalable tool for incrementally and idempotently loading data\nfrom cloud object storage and data lakes into the data lakehouse.\n\n## Machine learning, AI, and data science\n\nDatabricks machine learning expands the core functionality of the platform\nwith a suite of tools tailored to the needs of data scientists and ML\nengineers, including [MLflow](../mlflow/index.html) and [Databricks Runtime\nfor Machine Learning](../machine-learning/index.html).\n\n### Large language models and generative AI\n\nDatabricks Runtime for Machine Learning includes libraries like [Hugging Face\nTransformers](../machine-learning/train-model/huggingface/index.html) that\nallow you to integrate existing pre-trained models or other open-source\nlibraries into your workflow. The Databricks MLflow integration makes it easy\nto use the MLflow tracking service with transformer pipelines, models, and\nprocessing components. In addition, you can integrate\n[OpenAI](https://platform.openai.com/docs/introduction) models or solutions\nfrom partners like [John Snow Labs](../machine-learning/reference-\nsolutions/natural-language-processing.html#john-snow-labs) in your Databricks\nworkflows.\n\nWith Databricks, you can customize a LLM on your data for your specific task.\nWith the support of open source tooling, such as Hugging Face and DeepSpeed,\nyou can efficiently take a foundation LLM and start training with your own\ndata to have more accuracy for your domain and workload.\n\nIn addition, Databricks provides AI functions that SQL data analysts can use\nto access LLM models, including from OpenAI, directly within their data\npipelines and workflows. See [AI Functions on Databricks](../large-language-\nmodels/ai-functions.html).\n\n## Data warehousing, analytics, and BI\n\nDatabricks combines user-friendly UIs with cost-effective compute resources\nand infinitely scalable, affordable storage to provide a powerful platform for\nrunning analytic queries. Administrators configure scalable compute clusters\nas [SQL warehouses](../compute/sql-warehouse/index.html), allowing end users\nto execute queries without worrying about any of the complexities of working\nin the cloud. SQL users can run queries against data in the lakehouse using\nthe [SQL query editor](../sql/user/sql-editor/index.html) or in notebooks.\n[Notebooks](../notebooks/index.html) support Python, R, and Scala in addition\nto SQL, and allow users to embed the same\n[visualizations](../visualizations/index.html) available in [legacy\ndashboards](../sql/user/dashboards/index.html) alongside links, images, and\ncommentary written in markdown.\n\n## Data governance and secure data sharing\n\nUnity Catalog provides a unified data governance model for the data lakehouse.\nCloud administrators configure and integrate coarse access control permissions\nfor Unity Catalog, and then Databricks administrators can manage permissions\nfor teams and individuals. Privileges are managed with access control lists\n(ACLs) through either user-friendly UIs or SQL syntax, making it easier for\ndatabase administrators to secure access to data without needing to scale on\ncloud-native identity access management (IAM) and networking.\n\nUnity Catalog makes running secure analytics in the cloud simple, and provides\na division of responsibility that helps limit the reskilling or upskilling\nnecessary for both administrators and end users of the platform. See [What is\nUnity Catalog?](../data-governance/unity-catalog/index.html).\n\nThe lakehouse makes data sharing within your organization as simple as\ngranting query access to a table or view. For sharing outside of your secure\nenvironment, Unity Catalog features a managed version of [Delta\nSharing](../delta-sharing/index.html).\n\n## DevOps, CI/CD, and task orchestration\n\nThe development lifecycles for ETL pipelines, ML models, and analytics\ndashboards each present their own unique challenges. Databricks allows all of\nyour users to leverage a single data source, which reduces duplicate efforts\nand out-of-sync reporting. By additionally providing a suite of common tools\nfor versioning, automating, scheduling, deploying code and production\nresources, you can simplify your overhead for monitoring, orchestration, and\noperations. [Jobs](../jobs/index.html) schedule Databricks notebooks, SQL\nqueries, and other arbitrary code. [Git folders](../repos/index.html) let you\nsync Databricks projects with a number of popular git providers. For a\ncomplete overview of tools, see [Developer tools](../dev-tools/index.html).\n\n## Real-time and streaming analytics\n\nDatabricks leverages Apache Spark Structured Streaming to work with streaming\ndata and incremental data changes. Structured Streaming integrates tightly\nwith Delta Lake, and these technologies provide the foundations for both Delta\nLive Tables and Auto Loader. See [Streaming on Databricks](../structured-\nstreaming/index.html).\n\n* * *\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the\nSpark logo are trademarks of the [Apache Software\nFoundation](http://www.apache.org/).\n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback) | [Privacy Policy](https://databricks.com/privacy-policy) | [Terms of Use](https://databricks.com/terms-of-use)\n\n### In this article:\n\n  * How does a data intelligence platform work?\n  * What is Databricks used for?\n  * Managed integration with open source\n  * Tools and programmatic access\n  * How does Databricks work with AWS?\n  * What are common use cases for Databricks?\n  * Build an enterprise data lakehouse\n  * ETL and data engineering\n  * Machine learning, AI, and data science\n  * Data warehousing, analytics, and BI\n  * Data governance and secure data sharing\n  * DevOps, CI/CD, and task orchestration\n  * Real-time and streaming analytics",
                outbound_links=[
                    ArtifactSummary(
                        artifact_id="74070d97-6312-402d-bbf9-fe0508797c9f",
                        url="https://docs.databricks.com/en/compute/sql-warehouse/index.html",
                        title="Connect to a SQL warehouse | Databricks on AWS",
                        summary="This article provides an overview of SQL warehouses in Databricks, which serve as compute resources for querying and exploring data. It explains the benefits of using SQL warehouses, including serverless options, management features, and connectivity to third-party BI tools. Additionally, the article outlines how to start and create SQL warehouses, their sizing and autoscaling behavior, and how they compare to SQL endpoints.",
                    ),
                    ArtifactSummary(
                        artifact_id="34b947ce-5417-4d8a-a86f-8682df13d5d2",
                        url="https://docs.databricks.com/en/lakehouse/index.html",
                        title="What is a data lakehouse? | Databricks on AWS",
                        summary="A data lakehouse is a data management system that merges the advantages of data lakes and data warehouses. This article outlines the lakehouse architectural pattern, its functionality, and its benefits on the Databricks platform. It highlights how data lakehouses can help organizations streamline their data processing workflows and maintain a single source of truth, thereby enhancing data freshness and reducing costs.",
                    ),
                    ArtifactSummary(
                        artifact_id="c7a8d84d-fe29-4e61-89be-f0187863db4d",
                        url="https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html",
                        title="What is Auto Loader? | Databricks on AWS",
                        summary="The article provides an overview of Auto Loader, a feature of Databricks that enables the incremental and efficient processing of new data files as they arrive in cloud storage. It explains how Auto Loader works, its supported sources, and its benefits over traditional Structured Streaming methods. This resource is essential for users looking to understand and implement Auto Loader for data ingestion into Databricks.",
                    ),
                    ArtifactSummary(
                        artifact_id="5ffe8303-aca6-47e8-b978-cab67fd6b005",
                        url="https://docs.databricks.com/en/delta-live-tables/index.html",
                        title="What is Delta Live Tables? | Databricks on AWS",
                        summary="Delta Live Tables is a declarative framework designed to build reliable and maintainable data processing pipelines. It simplifies the management of data transformations by automating task orchestration, cluster management, and monitoring. Users can enforce data quality and handle errors effectively while focusing on defining the transformations needed for their datasets.",
                    ),
                    ArtifactSummary(
                        artifact_id="15d6ff2f-41c2-41ae-ba68-5071e6128a83",
                        url="https://docs.databricks.com/en/machine-learning/train-model/huggingface/index.html",
                        title="What are Hugging Face Transformers? | Databricks on AWS",
                        summary="This article provides an introduction to Hugging Face Transformers on Databricks. It includes guidance on why to use Hugging Face Transformers and how to install it on your cluster. The article covers the background of Hugging Face Transformers, installation steps, and resources for single node training and model dependencies.",
                    ),
                    ArtifactSummary(
                        artifact_id="cf3deb02-7ebd-4d07-a7c0-ef8db057fd2b",
                        url="https://docs.databricks.com/en/ingestion/index.html",
                        title="Ingest data into a Databricks lakehouse | Databricks on AWS",
                        summary="This article provides a comprehensive guide on how to ingest data into a Databricks lakehouse, which is backed by Delta Lake. It outlines various data sources and offers links to detailed steps for ingesting data from each source type, including cloud storage, enterprise applications, streaming sources, and local files. The goal is to help users efficiently manage and migrate their data into a Databricks environment.",
                    ),
                    ArtifactSummary(
                        artifact_id="e413ee85-f345-4c75-a3df-2ec9cd5c5163",
                        url="https://docs.databricks.com/en/sql/index.html",
                        title="What is data warehousing on Databricks? | Databricks on AWS",
                        summary="Data warehousing involves collecting and storing data from various sources to enable quick access for business insights and reporting. This article explains key concepts for building a data warehouse within a data lakehouse framework on Databricks. It covers how Databricks SQL provides data warehousing capabilities and discusses the medallion architecture for data modeling, highlighting the importance of different data layers for effective data management.",
                    ),
                    ArtifactSummary(
                        artifact_id="38c68177-24e2-4b28-aef0-518dfa2c59d0",
                        url="https://docs.databricks.com/en/repos/index.html",
                        title="Git integration for Databricks Git folders | Databricks on AWS",
                        summary="This article discusses the Databricks Git folders feature, which serves as a visual Git client and API within the Databricks environment. It enables users to perform common Git operations such as cloning repositories, committing changes, and managing branches. The integration with Git facilitates version control, collaboration, and CI/CD practices for data science and engineering projects.",
                    ),
                    ArtifactSummary(
                        artifact_id="6889ed2b-e3a0-4119-be89-d770a4397a3b",
                        url="https://docs.databricks.com/en/visualizations/index.html",
                        title="Visualizations in Databricks notebooks | Databricks on AWS",
                        summary="This article provides guidance on how to create and work with visualizations in Databricks notebooks. It covers the process of creating new visualizations, data profiles, and how to manage these visualizations effectively. Additionally, it highlights various tools available for enhancing visualizations and integrating them into dashboards.",
                    ),
                    ArtifactSummary(
                        artifact_id="5c1dba77-371f-4560-9c5f-f2b3443b7aab",
                        url="https://docs.databricks.com/en/machine-learning/index.html",
                        title="AI and machine learning on Databricks | Databricks on AWS",
                        summary="This article describes the tools that Mosaic AI (formerly Databricks Machine Learning) provides to help you build AI and ML systems. It outlines how various products on the Databricks platform facilitate the implementation of end-to-end workflows for building and deploying AI and ML systems. The article also highlights key features and components that unite the AI lifecycle, including generative AI, machine learning, and deep learning applications.",
                    ),
                    ArtifactSummary(
                        artifact_id="f2f27422-d3f0-4ade-88af-042f6c21d64c",
                        url="https://docs.databricks.com/en/compute/photon.html",
                        title="What is Photon? | Databricks on AWS",
                        summary="This article explains the benefits of running workloads on the Photon query engine, a high-performance Databricks-native vectorized query engine designed to accelerate SQL workloads and DataFrame API calls. Photon aims to reduce the total cost per workload while being compatible with existing Apache Spark APIs. Key topics include Photon features, how to enable it, supported operators, and limitations.",
                    ),
                    ArtifactSummary(
                        artifact_id="97efc518-83be-481a-bff6-f1f41692e341",
                        url="https://docs.databricks.com/en/notebooks/index.html",
                        title="Introduction to Databricks notebooks | Databricks on AWS",
                        summary="This article provides an overview of Databricks notebooks, which are essential tools for data science and machine learning. It explains how to use these notebooks for developing code, collaborating with others, and presenting results. The article also details various features of Databricks notebooks, including real-time coauthoring, automatic versioning, and integration with various programming languages.",
                    ),
                    ArtifactSummary(
                        artifact_id="3ddebff2-c026-4605-aa0b-2287c4145f25",
                        url="https://docs.databricks.com/en/sql/user/dashboards/index.html",
                        title="Legacy dashboards | Databricks on AWS",
                        summary="Legacy dashboards in Databricks allow users to combine visualizations and text boxes to provide context around their data. Although they can still be used, Databricks recommends transitioning to AI/BI dashboards. This guide explains how to view, create, edit, and manage legacy dashboards, including cloning and refreshing them. It also covers the permissions and ownership configurations necessary for effective dashboard management.",
                    ),
                    ArtifactSummary(
                        artifact_id="423ce4bf-03c0-40e4-a755-1e110eabbbcc",
                        url="https://docs.databricks.com/en/jobs/index.html",
                        title="Schedule and orchestrate workflows | Databricks on AWS",
                        summary="This article provides an overview of Databricks Workflows and how they can be used to schedule and orchestrate data processing tasks on Databricks. It covers key concepts such as Databricks Jobs, the necessary configurations for running jobs, and the various control flow options available. Additionally, it includes information on monitoring jobs, understanding limitations, and managing workflows programmatically.",
                    ),
                    ArtifactSummary(
                        artifact_id="9219bc78-3540-4239-a5a0-8eb0024982ff",
                        url="https://docs.databricks.com/en/dev-tools/index.html",
                        title="Developer tools | Databricks on AWS",
                        summary="This article provides an overview of the developer tools available in the Databricks ecosystem, which assist in developing applications and managing resources efficiently. It highlights the various tools offered, their functionalities, and recommendations for the best tools to use in different developer scenarios. By following this guide, developers can choose the right tools to enhance their productivity and streamline their workflows when working with Databricks.",
                    ),
                    ArtifactSummary(
                        artifact_id="6d997953-33a8-4cd4-967c-832e46e89b39",
                        url="https://docs.databricks.com/en/delta-sharing/index.html",
                        title="What is Delta Sharing? | Databricks on AWS",
                        summary="This article introduces Delta Sharing in Databricks, a secure data sharing platform that allows users to share data and AI assets with individuals outside their organization, regardless of whether those users utilize Databricks. Delta Sharing also forms the foundation for the Databricks Marketplace, an open forum for exchanging data products, and Clean Rooms, which provide a secure environment for collaborative work on sensitive data. Additionally, it is available as an open-source project for sharing Delta tables across different platforms.",
                    ),
                    ArtifactSummary(
                        artifact_id="630b44b6-05b7-4f95-91ed-c45e1b61a0ca",
                        url="https://docs.databricks.com/en/mlflow/index.html",
                        title="ML lifecycle management using MLflow | Databricks on AWS",
                        summary="This article describes how MLflow is used in Databricks for machine learning lifecycle management. It provides an overview of the primary components of MLflow and includes examples that demonstrate how these components are integrated within Databricks. The article is aimed at both first-time users and experienced practitioners, offering insights into tracking experiments, managing models, and deploying them effectively.",
                    ),
                    ArtifactSummary(
                        artifact_id="a654ae3f-6d91-4eae-9afd-7f0a6fa08408",
                        url="https://docs.databricks.com/en/sql/user/sql-editor/index.html",
                        title="Write queries and explore data in the SQL editor | Databricks on AWS",
                        summary="This article provides an overview of how to use the SQL editor within Databricks to write and manage SQL queries. It covers the main features of the SQL editor, including connecting to compute resources, browsing data, creating and saving queries, and sharing them with team members. The article also highlights new functionalities available in the Public Preview version of the SQL editor.",
                    ),
                    ArtifactSummary(
                        artifact_id="55496a60-abed-4bf1-a74f-8bd25b064698",
                        url="https://docs.databricks.com/en/data-governance/unity-catalog/index.html",
                        title="What is Unity Catalog? | Databricks on AWS",
                        summary="This article introduces Unity Catalog, a unified governance solution for data and AI assets on Databricks. It covers key features such as centralized access control, auditing, lineage, and data discovery capabilities. Additionally, it explains the object model of Unity Catalog, how to work with database objects, manage access, and the requirements for setting up Unity Catalog in an organization.",
                    ),
                ],
                inbound_links=[
                    ArtifactSummary(
                        artifact_id="0f76a039-c6be-4ed0-865a-b046b7e2cf29",
                        url="https://docs.databricks.com/en/resources/glossary.html",
                        title="Databricks technical terminology glossary | Databricks on AWS",
                        summary="This glossary serves as a comprehensive reference for technical terminology related to Databricks, particularly in the context of cloud computing and data management. It provides definitions and explanations of various terms, concepts, and features associated with the Databricks platform, aiding users in understanding and utilizing the platform effectively. The glossary covers a wide range of topics, from basic terms to advanced concepts in data engineering, machine learning, and cloud services.",
                    ),
                    ArtifactSummary(
                        artifact_id="eba1164f-4617-48e4-a013-292aa83ef0c0",
                        url="https://docs.databricks.com/en/delta-sharing/recipient.html",
                        title="Access data shared with you using Delta Sharing (for recipients) | Databricks on AWS",
                        summary="This article provides guidance on how to access data that has been shared with you using Delta Sharing. It explains the concepts of Delta Sharing and data recipients, detailing the steps to gain access to shared data whether through Databricks-to-Databricks or open sharing methods. The article also covers how to read the shared data and audit its usage, along with next steps for further learning.",
                    ),
                    ArtifactSummary(
                        artifact_id="7ef41660-b41c-4656-958c-ba448d9f2764",
                        url="https://docs.databricks.com/en/data-engineering.html",
                        title="Data engineering with Databricks | Databricks on AWS",
                        summary="Databricks provides a comprehensive suite of data engineering features that facilitate collaboration among data professionals. It focuses on efficient data pipelines through tools like Structured Streaming and Delta Live Tables. The article also highlights additional resources and tools available for data engineering tasks, including notebooks and workflow orchestration.",
                    ),
                    ArtifactSummary(
                        artifact_id="485279b9-2b2f-4c31-8888-46f93e9cef47",
                        url="https://docs.databricks.com/en/migration/index.html",
                        title="Migrate data applications to Databricks | Databricks on AWS",
                        summary="This article provides an introduction to migrating existing data applications to Databricks. Databricks offers a unified platform for working with data from various source systems, facilitating the transition from traditional data architectures to a more streamlined lakehouse model. It covers key aspects such as migrating ETL jobs, replacing enterprise data warehouses, and unifying machine learning and analytics workloads.",
                    ),
                ],
            ),
            ArtifactWithLinks(
                artifact_id="263b9b55-023b-4315-867b-3d393cfd7707",
                url="https://docs.databricks.com/en/getting-started/index.html",
                title="Get started with Databricks | Databricks on AWS",
                summary="This article serves as a comprehensive guide for those who are new to Databricks. It includes instructions for setting up an account, navigating the Databricks workspace, and provides tutorials on exploratory data analysis and ETL processes. Additionally, it offers insights into machine learning capabilities within Databricks, making it an essential resource for beginners looking to leverage the platform effectively.",
                parsed_text="* __[](https://www.databricks.com/)\n\n  * __[](https://www.databricks.com/)\n  * [Help Center](https://help.databricks.com/s/)\n  * [Documentation](https://docs.databricks.com/en/index.html)\n  * [Knowledge Base](https://kb.databricks.com/)\n\n  * [Community](https://community.databricks.com)\n  * [Support](https://help.databricks.com)\n  * [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n  * [Try Databricks](https://databricks.com/try-databricks)\n\n[English](javascript:void\\(0\\))\n\n  * [日本語](../../ja/getting-started/index.html)\n  * [Português](../../pt/getting-started/index.html)\n\n[Amazon Web Services](javascript:void\\(0\\))\n\n  * [Microsoft Azure](https://learn.microsoft.com/en/azure/databricks/getting-started/)\n  * [Google Cloud Platform](https://docs.gcp.databricks.com/en/getting-started/index.html)\n\n[Databricks on AWS](../index.html)\n\nGet started\n\n  * Get started\n    * [Sign up for a free trial](free-trial.html)\n    * [Basic workspace setup](onboarding-account.html)\n    * [Understand your workspace](../workspace/index.html)\n    * [Exploratory data analysis (EDA)](quick-start.html)\n    * [Extract, transform, and load data (ETL)](etl-quick-start.html)\n    * [Machine learning](ml-get-started.html)\n  * [What is Databricks?](../introduction/index.html)\n  * [DatabricksIQ](../databricksiq/index.html)\n  * [Release notes](../release-notes/index.html)\n\nLoad & manage data\n\n  * [Work with database objects](../database-objects/index.html)\n  * [Connect to data sources](../connect/index.html)\n  * [Connect to compute](../compute/index.html)\n  * [Discover data](../discover/index.html)\n  * [Query data](../query/index.html)\n  * [Ingest data](../ingestion/index.html)\n  * [Work with files](../files/index.html)\n  * [Transform data](../transform/index.html)\n  * [Schedule and orchestrate workflows](../jobs/index.html)\n  * [Monitor data and AI assets](../lakehouse-monitoring/index.html)\n  * [Share data securely](../data-sharing/index.html)\n\nWork with data\n\n  * [Data engineering](../data-engineering.html)\n  * [AI and machine learning](../machine-learning/index.html)\n  * [Generative AI tutorial](../generative-ai/tutorials/ai-cookbook/index.html)\n  * [Business intelligence](../ai-bi/index.html)\n  * [Data warehousing](../sql/index.html)\n  * [Notebooks](../notebooks/index.html)\n  * [Delta Lake](../delta/index.html)\n  * [Developers](../languages/index.html)\n  * [Technology partners](../integrations/index.html)\n\nAdministration\n\n  * [Account and workspace administration](../admin/index.html)\n  * [Security and compliance](../security/index.html)\n  * [Data governance (Unity Catalog)](../data-governance/index.html)\n  * [Lakehouse architecture](../lakehouse-architecture/index.html)\n\nReference & resources\n\n  * [Reference](../reference/api.html)\n  * [Resources](../resources/index.html)\n  * [What’s coming?](../whats-coming.html)\n  * [Documentation archive](../archive/index.html)\n\nUpdated Nov 14, 2024\n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation\nFeedback)\n\n  * [Documentation](../index.html)\n  * Get started with Databricks\n  * \n\n# Get started with Databricks\n\nNovember 04, 2024\n\nIf you’re new to Databricks, you’ve found the place to start. This section\nincludes instructions for basic account setup, a tour of the Databricks\nworkspace UI, and some basic tutorials related to exploratory data analysis\nand ETL on Databricks.\n\nFor information about online training resources, see [Get free Databricks\ntraining](free-training.html).\n\n  * [Sign up for a free trial](free-trial.html)\n\nLearn how to set up a Databricks free trial.\n\n  * [Basic workspace setup](onboarding-account.html)\n\nLearn how to connect your new workspace to computing resources and your cloud\ndata storage.\n\n  * [Understand your workspace](../workspace/index.html)\n\nLearn how to navigate a Databricks workspace and access features using the\nDatabricks unified navigation experience.\n\n  * [Exploratory data analysis (EDA)](quick-start.html)\n\nLearn data science basics on Databricks. Using a notebook, query and visualize\ndata stored in Unity Catalog by using SQL, Python, Scala, and R.\n\n  * [Extract, transform, and load data (ETL)](etl-quick-start.html)\n\nLearn how to use Databricks to quickly develop and deploy your first ETL\npipeline for data orchestration.\n\n  * [Machine learning](ml-get-started.html)\n\nLearn how to build a simple machine learning classification model on\nDatabricks using the scikit-learn library.\n\n## Get help\n\n  * If you have any questions about setting up Databricks and need live help, please e-mail [onboarding-help@databricks.com](mailto:onboarding-help%40databricks.com).\n\n  * If you have a Databricks support package, you can open and manage support cases with Databricks. See [Learn how to use Databricks support](../resources/support.html).\n\n  * If your organization does not have a Databricks support subscription, or if you are not an authorized contact for your company’s support subscription, you can get answers to many questions in [Databricks Office Hours](https://www.databricks.com/p/webinar/officehours?utm_source=databricks&utm_medium=site&utm_content=docs) or from the [Databricks Community](https://community.databricks.com).\n\n  * If you need additional help, [sign up for a live weekly demo](https://databricks.com/p/databricks-weekly-demo) to ask questions and practice alongside Databricks experts. Or, follow this [blog series on best practices for managing and maintaining your environments](https://databricks.com/blog/2022/03/10/functional-workspace-organization-on-databricks.html).\n\n* * *\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the\nSpark logo are trademarks of the [Apache Software\nFoundation](http://www.apache.org/).\n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback) | [Privacy Policy](https://databricks.com/privacy-policy) | [Terms of Use](https://databricks.com/terms-of-use)",
                outbound_links=[
                    ArtifactSummary(
                        artifact_id="d803a38f-ecfc-4f65-aaf3-520c4a9748ba",
                        url="https://docs.databricks.com/en/getting-started/quick-start.html",
                        title="Get started: Query and visualize data from a notebook | Databricks on AWS",
                        summary="This article provides a step-by-step guide on how to use a Databricks notebook to query sample data from Unity Catalog with various programming languages such as SQL, Python, Scala, and R. It outlines the requirements for setting up your workspace and describes how to create a notebook, query a data table, and visualize the results. By following this guide, users can effectively interact with and visualize data within the Databricks environment.",
                    ),
                    ArtifactSummary(
                        artifact_id="73394f41-7bc6-4706-b71d-0b789fbff841",
                        url="https://docs.databricks.com/en/getting-started/free-trial.html",
                        title="Start a Databricks free trial | Databricks on AWS",
                        summary="This article provides an overview of how to start a free trial of Databricks on AWS, outlining the differences between signing up directly through Databricks and AWS Marketplace. It details the steps required for each sign-up method, how to manage billing and subscription details, and options for canceling the trial. The article aims to help users navigate the process of getting started with Databricks effectively and manage their accounts after the trial period.",
                    ),
                    ArtifactSummary(
                        artifact_id="6f308a0c-381b-423a-a800-3a0f033a96d3",
                        url="https://docs.databricks.com/en/getting-started/free-training.html",
                        title="Get free Databricks training | Databricks on AWS",
                        summary="This article provides information about free training offerings available for Databricks customers. It includes various resources such as courses, recorded webinars, and tutorials aimed at helping users understand and utilize the Databricks platform effectively. Customers can access these materials through their Databricks workspace account or by creating a new account.",
                    ),
                    ArtifactSummary(
                        artifact_id="aacd9736-ece9-4998-b27a-0746ba91d377",
                        url="https://docs.databricks.com/en/getting-started/etl-quick-start.html",
                        title="Run your first ETL workload on Databricks | Databricks on AWS",
                        summary="This article guides users through the process of developing and deploying their first ETL pipelines using Databricks. It covers essential steps such as launching a compute cluster, creating notebooks, configuring data ingestion with Auto Loader, and scheduling jobs. By the end of the tutorial, users will be equipped with the knowledge to perform common ETL tasks in Python or Scala, utilizing Databricks' interactive notebooks and Delta Lake.",
                    ),
                    ArtifactSummary(
                        artifact_id="e1748839-9037-4c1a-ab24-cb116880f2e9",
                        url="https://docs.databricks.com/en/getting-started/onboarding-account.html",
                        title="Get started: Databricks workspace onboarding | Databricks on AWS",
                        summary="This article provides a comprehensive 30-minute setup guide for creating and onboarding your first Databricks workspace. It outlines the necessary steps to establish a workspace, set up compute resources, connect to data sources, and manage user access. By following these instructions, users can quickly get started with Databricks and begin utilizing its powerful data processing capabilities.",
                    ),
                    ArtifactSummary(
                        artifact_id="e8a21f5f-6da8-4894-b9e2-a8ce493e0d89",
                        url="https://docs.databricks.com/en/workspace/index.html",
                        title="Navigate the workspace | Databricks on AWS",
                        summary="This article walks you through the Databricks workspace UI, an environment for accessing all of your Databricks objects. It covers how to manage the workspace using the UI, Databricks CLI, and Workspace API, and highlights common tasks within the workspace. The article aims to help users onboard and navigate effectively within the Databricks Data Intelligence Platform.",
                    ),
                    ArtifactSummary(
                        artifact_id="be1396dc-a0a0-4caa-9ad8-07f0d005140b",
                        url="https://docs.databricks.com/en/resources/support.html",
                        title="Support | Databricks on AWS",
                        summary="This article explains how users with dedicated Databricks support can open and manage support cases. It provides essential information for organizations that have a Databricks support subscription, including how to log in to the Help Center, create and manage support cases, and perform administrative tasks. Additionally, users can find guidance on escalating cases and updating their profiles for better support communication.",
                    ),
                    ArtifactSummary(
                        artifact_id="65f711b7-5b92-4cf9-b9c4-aab66a04159f",
                        url="https://docs.databricks.com/en/getting-started/ml-get-started.html",
                        title="Get started: Build your first machine learning model on Databricks | Databricks on AWS",
                        summary="This article guides users through the process of building a machine learning classification model using the `scikit-learn` library on Databricks. The primary aim is to create a model that predicts whether a wine is of 'high quality' based on various features. It also covers the use of MLflow for tracking model development and Hyperopt for automating hyperparameter tuning, making it a comprehensive resource for beginners in machine learning on Databricks.",
                    ),
                ],
                inbound_links=[
                    ArtifactSummary(
                        artifact_id="7ef41660-b41c-4656-958c-ba448d9f2764",
                        url="https://docs.databricks.com/en/data-engineering.html",
                        title="Data engineering with Databricks | Databricks on AWS",
                        summary="Databricks provides a comprehensive suite of data engineering features that facilitate collaboration among data professionals. It focuses on efficient data pipelines through tools like Structured Streaming and Delta Live Tables. The article also highlights additional resources and tools available for data engineering tasks, including notebooks and workflow orchestration.",
                    ),
                    ArtifactSummary(
                        artifact_id="6f308a0c-381b-423a-a800-3a0f033a96d3",
                        url="https://docs.databricks.com/en/getting-started/free-training.html",
                        title="Get free Databricks training | Databricks on AWS",
                        summary="This article provides information about free training offerings available for Databricks customers. It includes various resources such as courses, recorded webinars, and tutorials aimed at helping users understand and utilize the Databricks platform effectively. Customers can access these materials through their Databricks workspace account or by creating a new account.",
                    ),
                    ArtifactSummary(
                        artifact_id="d803a38f-ecfc-4f65-aaf3-520c4a9748ba",
                        url="https://docs.databricks.com/en/getting-started/quick-start.html",
                        title="Get started: Query and visualize data from a notebook | Databricks on AWS",
                        summary="This article provides a step-by-step guide on how to use a Databricks notebook to query sample data from Unity Catalog with various programming languages such as SQL, Python, Scala, and R. It outlines the requirements for setting up your workspace and describes how to create a notebook, query a data table, and visualize the results. By following this guide, users can effectively interact with and visualize data within the Databricks environment.",
                    ),
                    ArtifactSummary(
                        artifact_id="5c7dc16b-e90b-413c-a681-e8fedfc093f5",
                        url="https://docs.databricks.com/en/getting-started/ingest-insert-additional-data.html",
                        title="Get started: Ingest and insert additional data | Databricks on AWS",
                        summary="This article provides a comprehensive guide on how to ingest and insert additional data into your Unity Catalog using Databricks. It walks you through utilizing a Databricks notebook to ingest a CSV file with baby name data and import it into an existing table using Python, Scala, and R. The article outlines the necessary requirements and step-by-step instructions for each process involved in the data ingestion workflow.",
                    ),
                    ArtifactSummary(
                        artifact_id="643e5805-39a5-438e-a64c-1973b5ab69df",
                        url="https://docs.databricks.com/en/getting-started/cleanse-enhance-data.html",
                        title="Get started: Enhance and cleanse data | Databricks on AWS",
                        summary="This article provides a step-by-step guide on using a Databricks notebook to cleanse and enhance the New York State baby name data. It covers tasks such as changing column names, adjusting capitalization, and filtering data for analysis. The final output includes saving the transformed data into silver and gold tables, with visualizations to enhance understanding.",
                    ),
                    ArtifactSummary(
                        artifact_id="4cb1c3a4-f2fd-4059-98a6-0793868e3add",
                        url="https://docs.databricks.com/en/mlflow/access-hosted-tracking-server.html",
                        title="Access the MLflow tracking server from outside Databricks | Databricks on AWS",
                        summary="This article provides a detailed guide on how to log to the MLflow tracking server from applications or the MLflow CLI outside of Databricks. It outlines the necessary configuration steps, including installing MLflow and configuring authentication. The article also highlights how to set up MLflow applications and the CLI for logging to a Databricks-hosted tracking server.",
                    ),
                    ArtifactSummary(
                        artifact_id="0bb9f43d-ea15-4cff-b8cd-05a0e93248c3",
                        url="https://docs.databricks.com/en/connect/external-systems/synapse-analytics-dedicated-pool.html",
                        title="Connect to Azure Synapse Analytics dedicated pool | Databricks on AWS",
                        summary="This tutorial provides a comprehensive guide on connecting Azure Databricks to Azure Synapse Analytics dedicated pool using various authentication methods such as service principal, Azure Managed Service Identity (MSI), and SQL Authentication. It covers the necessary requirements and detailed steps to establish the connection, including creating a Microsoft Entra ID service principal, setting up necessary permissions, and troubleshooting common issues. By following this guide, users can effectively integrate Azure Databricks with Azure Synapse Analytics for enhanced data management and analysis.",
                    ),
                    ArtifactSummary(
                        artifact_id="0ed1008e-fdd7-40a5-9375-5b800fbbadef",
                        url="https://docs.databricks.com/en/getting-started/import-visualize-data.html",
                        title="Get started: Import and visualize CSV data from a notebook | Databricks on AWS",
                        summary="This article provides a comprehensive guide on how to import data from a CSV file into a Databricks notebook using Python, Scala, and R. It covers the necessary requirements to get started, step-by-step instructions for creating a notebook, defining variables, importing CSV data, visualizing the data, and saving it to a table in Unity Catalog. Users will learn how to manipulate data effectively and visualize it using various tools within Databricks.",
                    ),
                    ArtifactSummary(
                        artifact_id="270bc981-5d13-4dd5-bab2-d218081a9f45",
                        url="https://docs.databricks.com/en/ingestion/cloud-object-storage/copy-into/tutorial-notebook.html",
                        title="Tutorial: COPY INTO with Spark SQL | Databricks on AWS",
                        summary="This tutorial guides users through the process of using the COPY INTO command in Databricks for loading data from cloud object storage into a Databricks table. It covers requirements, step-by-step instructions for setting up the environment, writing sample data, and using COPY INTO for efficient data ingestion. Additionally, it explains how to preview the contents of the table and clean up resources after the tutorial is complete.",
                    ),
                    ArtifactSummary(
                        artifact_id="18392891-42de-48ad-90ad-e0a0af0e0ffd",
                        url="https://docs.databricks.com/en/getting-started/admin-get-started.html",
                        title="Get started with Databricks administration | Databricks on AWS",
                        summary="This article provides comprehensive guidance for new account and workspace administrators who want to leverage the administrative and security features of Databricks. It outlines essential steps for setting up an account, managing permissions, and implementing security measures. Additionally, it highlights the resources available for support and further learning, ensuring admins are well-equipped to manage their Databricks environment effectively.",
                    ),
                    ArtifactSummary(
                        artifact_id="1a93d28e-4477-4933-9044-ced3ad8e2dd3",
                        url="https://docs.databricks.com/en/archive/admin-guide/account.html",
                        title="Manage your subscription (legacy) | Databricks on AWS",
                        summary="This article provides guidance on managing your Databricks subscription using the legacy account console. It includes important information regarding billing, upgrading subscriptions, and account deletion. Users should be aware that this documentation has been retired, indicating that the features discussed may not be supported anymore, and legacy workspaces will be retired on December 31st, 2023.",
                    ),
                ],
            ),
            ArtifactWithLinks(
                artifact_id="e4d26a2c-fcba-4387-9e22-eff18b18a14b",
                url="https://docs.databricks.com/en/getting-started/connect/index.html",
                title="Databricks integrations overview | Databricks on AWS",
                summary="This article provides an overview of how to connect various data sources, BI tools, and developer tools to Databricks. It discusses the available integrations through Partner Connect and outlines the types of data formats and ETL tools that can be utilized with Databricks. Additionally, it highlights support for IDEs and Git integration, offering a comprehensive guide for users looking to enhance their data workflows using Databricks.",
                parsed_text="* __[](https://www.databricks.com/)\n\n  * __[](https://www.databricks.com/)\n  * [Help Center](https://help.databricks.com/s/)\n  * [Documentation](https://docs.databricks.com/en/index.html)\n  * [Knowledge Base](https://kb.databricks.com/)\n\n  * [Community](https://community.databricks.com)\n  * [Support](https://help.databricks.com)\n  * [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n  * [Try Databricks](https://databricks.com/try-databricks)\n\n[English](javascript:void\\(0\\))\n\n  * [日本語](../../../ja/getting-started/connect/index.html)\n  * [Português](../../../pt/getting-started/connect/index.html)\n\n[Amazon Web Services](javascript:void\\(0\\))\n\n  * [Microsoft Azure](https://learn.microsoft.com/en/azure/databricks/getting-started/connect/)\n  * [Google Cloud Platform](https://docs.gcp.databricks.com/en/getting-started/connect/index.html)\n\n[Databricks on AWS](../../index.html)\n\nGet started\n\n  * [Get started](../index.html)\n  * [What is Databricks?](../../introduction/index.html)\n    * [ What is a data lakehouse?](../../lakehouse/index.html)\n    * [ Apache Spark](../../spark/index.html)\n    * [ What is Delta?](../../introduction/delta-comparison.html)\n    * [ Concepts](../concepts.html)\n    * [ Architecture](../overview.html)\n    * Integrations\n  * [DatabricksIQ](../../databricksiq/index.html)\n  * [Release notes](../../release-notes/index.html)\n\nLoad & manage data\n\n  * [Work with database objects](../../database-objects/index.html)\n  * [Connect to data sources](../../connect/index.html)\n  * [Connect to compute](../../compute/index.html)\n  * [Discover data](../../discover/index.html)\n  * [Query data](../../query/index.html)\n  * [Ingest data](../../ingestion/index.html)\n  * [Work with files](../../files/index.html)\n  * [Transform data](../../transform/index.html)\n  * [Schedule and orchestrate workflows](../../jobs/index.html)\n  * [Monitor data and AI assets](../../lakehouse-monitoring/index.html)\n  * [Share data securely](../../data-sharing/index.html)\n\nWork with data\n\n  * [Data engineering](../../data-engineering.html)\n  * [AI and machine learning](../../machine-learning/index.html)\n  * [Generative AI tutorial](../../generative-ai/tutorials/ai-cookbook/index.html)\n  * [Business intelligence](../../ai-bi/index.html)\n  * [Data warehousing](../../sql/index.html)\n  * [Notebooks](../../notebooks/index.html)\n  * [Delta Lake](../../delta/index.html)\n  * [Developers](../../languages/index.html)\n  * [Technology partners](../../integrations/index.html)\n\nAdministration\n\n  * [Account and workspace administration](../../admin/index.html)\n  * [Security and compliance](../../security/index.html)\n  * [Data governance (Unity Catalog)](../../data-governance/index.html)\n  * [Lakehouse architecture](../../lakehouse-architecture/index.html)\n\nReference & resources\n\n  * [Reference](../../reference/api.html)\n  * [Resources](../../resources/index.html)\n  * [What’s coming?](../../whats-coming.html)\n  * [Documentation archive](../../archive/index.html)\n\nUpdated Nov 14, 2024\n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation\nFeedback)\n\n  * [Documentation](../../index.html)\n  * [What is Databricks?](../../introduction/index.html)\n  * Databricks integrations overview\n  * \n\n# Databricks integrations overview\n\nAugust 22, 2024\n\nThe articles listed here provide information about how to connect to the large\nassortment of data sources, BI tools, and developer tools that you can use\nwith Databricks. Many of these are available through our system of partners\nand our Partner Connect hub.\n\n**In this article:**\n\n  * Partner Connect\n  * Data sources\n  * BI tools\n  * Other ETL tools\n  * IDEs and other developer tools\n  * Git\n\n## Partner Connect\n\nPartner Connect is a user interface that allows validated solutions to\nintegrate more quickly and easily with your Databricks clusters and SQL\nwarehouses.\n\nFor more information, see [What is Databricks Partner Connect?](../../partner-\nconnect/index.html).\n\n## Data sources\n\nDatabricks can read data from and write data to a variety of data formats such\nas CSV, [Delta Lake](../../delta/index.html), JSON, Parquet, XML, and other\nformats, as well as data storage providers such as Amazon S3, Google BigQuery\nand Cloud Storage, Snowflake, and other providers.\n\nSee [Data ingestion](../../integrations/index.html#ingest), [Connect to data\nsources](../../connect/index.html), and [Data format\noptions](../../query/formats/index.html).\n\n## BI tools\n\nDatabricks has validated integrations with your favorite BI tools, including\nPower BI, Tableau, and others, allowing you to work with data through\nDatabricks clusters and SQL warehouses, in many cases with low-code and no-\ncode experiences.\n\nFor a comprehensive list, with connection instructions, see [BI and\nvisualization](../../integrations/index.html#bi).\n\n## Other ETL tools\n\nIn addition to access to all kinds of data sources, Databricks provides\nintegrations with ETL/ELT tools like dbt, Prophecy, and Azure Data Factory, as\nwell as data pipeline orchestration tools like Airflow and SQL database tools\nlike DataGrip, DBeaver, and SQL Workbench/J.\n\nFor connection instructions, see:\n\n  * **ETL tools** : [Data preparation and transformation](../../integrations/index.html#prep)\n\n  * **Airflow** : [Orchestrate Databricks jobs with Apache Airflow](../../jobs/how-to/use-airflow-with-jobs.html)\n\n  * **SQL database tools** : [SQL connectors, libraries, drivers, APIs, and tools](../../dev-tools/sql-drivers-tools.html).\n\n## IDEs and other developer tools\n\nDatabricks supports developer tools such as DataGrip, IntelliJ, PyCharm,\nVisual Studio Code, and others, that allow you to programmatically access\nDatabricks [compute](../../compute/index.html), including [SQL\nwarehouses](../../compute/sql-warehouse/index.html).\n\nFor a comprehensive list of tools that support developers, see [Develop on\nDatabricks](../../languages/index.html).\n\n## Git\n\nDatabricks Git folders provide repository-level integration with your favorite\nGit providers, so you can develop code in a Databricks notebook and sync it\nwith a remote Git repository. See [Git integration for Databricks Git\nfolders](../../repos/index.html).\n\n* * *\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the\nSpark logo are trademarks of the [Apache Software\nFoundation](http://www.apache.org/).\n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback) | [Privacy Policy](https://databricks.com/privacy-policy) | [Terms of Use](https://databricks.com/terms-of-use)\n\n### In this article:\n\n  * Partner Connect\n  * Data sources\n  * BI tools\n  * Other ETL tools\n  * IDEs and other developer tools\n  * Git",
                outbound_links=[
                    ArtifactSummary(
                        artifact_id="61f3b1a6-3215-46ce-bd77-995e4aed5518",
                        url="https://docs.databricks.com/en/jobs/how-to/use-airflow-with-jobs.html",
                        title="Orchestrate Databricks jobs with Apache Airflow | Databricks on AWS",
                        summary="This article describes the Apache Airflow support for orchestrating data pipelines with Databricks. It includes instructions for installing and configuring Airflow locally, as well as an example of deploying and running a Databricks workflow using Airflow. Readers will learn about the requirements, various Airflow operators for Databricks, and step-by-step guidance on creating a Databricks job integrated with Airflow.",
                    ),
                    ArtifactSummary(
                        artifact_id="f0ae9e85-9595-449a-a4ce-91d74445ce83",
                        url="https://docs.databricks.com/en/languages/index.html",
                        title="Develop on Databricks | Databricks on AWS",
                        summary="This page provides a comprehensive overview for developers looking to utilize Databricks across various programming languages such as Python, R, Scala, and SQL. It highlights the tools and features available to automate and enhance ETL pipelines and the software development lifecycle within the Databricks environment. The content is designed to help developers find the resources they need to effectively work with Databricks and integrate it into their workflows.",
                    ),
                    ArtifactSummary(
                        artifact_id="0dc8e82a-6027-4d45-b8a6-bb1305499e2d",
                        url="https://docs.databricks.com/en/delta/index.html",
                        title="What is Delta Lake? | Databricks on AWS",
                        summary="Delta Lake is the optimized storage layer that provides the foundation for tables in a lakehouse on Databricks. It extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling, making it fully compatible with Apache Spark APIs. This article covers various aspects of Delta Lake such as getting started, converting data, managing tables, and utilizing streaming workloads, ensuring users can leverage its features effectively.",
                    ),
                    ArtifactSummary(
                        artifact_id="1d7dbb3f-70e2-4812-ad05-662661f232e7",
                        url="https://docs.databricks.com/en/integrations/index.html",
                        title="Technology partners | Databricks on AWS",
                        summary="This article discusses the various technology partners that have validated integrations with Databricks on AWS. These integrations facilitate data management through Databricks clusters and SQL warehouses, offering low-code and no-code solutions for data ingestion, preparation, transformation, business intelligence, and machine learning. It also introduces Databricks Partner Connect, which simplifies the integration process for users.",
                    ),
                    ArtifactSummary(
                        artifact_id="1448e65e-9cd0-4328-a244-2b09d710f9ea",
                        url="https://docs.databricks.com/en/dev-tools/sql-drivers-tools.html",
                        title="SQL connectors, libraries, drivers, APIs, and tools | Databricks on AWS",
                        summary="This article provides an overview of the various SQL connectors, libraries, drivers, APIs, and tools available for Databricks on AWS. It highlights how these tools facilitate connections to Databricks and enable programmatic interaction with Databricks SQL functionality across popular programming languages like Python, Go, and JavaScript. The content aims to assist developers in integrating Databricks with their applications to streamline data operations.",
                    ),
                    ArtifactSummary(
                        artifact_id="c9f52fad-6eff-4982-81ab-1bae4f7d4959",
                        url="https://docs.databricks.com/en/connect/index.html",
                        title="Connect to data sources | Databricks on AWS",
                        summary="This article provides recommendations for administrators and power users on how to configure connections between Databricks and various data sources. It outlines the necessary privileges and methods for connecting to cloud object storage, relational databases, streaming data services, and more. Additionally, it offers guidance on how to request access to data sources if users lack the required permissions.",
                    ),
                    ArtifactSummary(
                        artifact_id="38c68177-24e2-4b28-aef0-518dfa2c59d0",
                        url="https://docs.databricks.com/en/repos/index.html",
                        title="Git integration for Databricks Git folders | Databricks on AWS",
                        summary="This article discusses the Databricks Git folders feature, which serves as a visual Git client and API within the Databricks environment. It enables users to perform common Git operations such as cloning repositories, committing changes, and managing branches. The integration with Git facilitates version control, collaboration, and CI/CD practices for data science and engineering projects.",
                    ),
                    ArtifactSummary(
                        artifact_id="15987fc1-4321-4b16-a800-54a5fe8b68d1",
                        url="https://docs.databricks.com/en/compute/index.html",
                        title="Compute | Databricks on AWS",
                        summary="The article discusses the various types of compute resources available in the Databricks workspace on AWS. It highlights the importance of compute in running data engineering, data science, and analytics workloads, including ETL pipelines and machine learning. Additionally, the article covers the Databricks Runtime, which includes essential components and features that enhance big data analytics.",
                    ),
                    ArtifactSummary(
                        artifact_id="74070d97-6312-402d-bbf9-fe0508797c9f",
                        url="https://docs.databricks.com/en/compute/sql-warehouse/index.html",
                        title="Connect to a SQL warehouse | Databricks on AWS",
                        summary="This article provides an overview of SQL warehouses in Databricks, which serve as compute resources for querying and exploring data. It explains the benefits of using SQL warehouses, including serverless options, management features, and connectivity to third-party BI tools. Additionally, the article outlines how to start and create SQL warehouses, their sizing and autoscaling behavior, and how they compare to SQL endpoints.",
                    ),
                    ArtifactSummary(
                        artifact_id="3519413a-5299-48b4-8a78-19f760ed4a5b",
                        url="https://docs.databricks.com/en/query/formats/index.html",
                        title="Data format options | Databricks on AWS",
                        summary="This article provides an overview of the various data format options available in Databricks, including how to read and write different data formats using Apache Spark. It details built-in keyword bindings for formats like Delta Lake, Parquet, JSON, CSV, and others, emphasizing Delta Lake as the default protocol. Additionally, the article highlights special considerations when working with certain data formats, ensuring users understand the configurations required for optimal usage.",
                    ),
                ],
                inbound_links=[],
            ),
            ArtifactWithLinks(
                artifact_id="ee044e97-b3e0-40de-a4a5-9098e21c5cc0",
                url="https://docs.databricks.com/en/dev-tools/bundles/deployment-modes.html",
                title="Databricks Asset Bundle deployment modes | Databricks on AWS",
                summary="This article describes the syntax for Databricks Asset Bundle deployment modes, which enable programmatic management of Databricks workflows. It outlines different deployment phases, including development and production modes, and offers guidance on how to implement these modes for effective workflow management. Additionally, it introduces the concept of custom presets that allow for further customization of deployment behaviors.",
                parsed_text='* __[](https://www.databricks.com/)\n\n  * __[](https://www.databricks.com/)\n  * [Help Center](https://help.databricks.com/s/)\n  * [Documentation](https://docs.databricks.com/en/index.html)\n  * [Knowledge Base](https://kb.databricks.com/)\n\n  * [Community](https://community.databricks.com)\n  * [Support](https://help.databricks.com)\n  * [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n  * [Try Databricks](https://databricks.com/try-databricks)\n\n[English](javascript:void\\(0\\))\n\n  * [日本語](../../../ja/dev-tools/bundles/deployment-modes.html)\n  * [Português](../../../pt/dev-tools/bundles/deployment-modes.html)\n\n[Amazon Web Services](javascript:void\\(0\\))\n\n  * [Microsoft Azure](https://learn.microsoft.com/en/azure/databricks/dev-tools/bundles/deployment-modes)\n  * [Google Cloud Platform](https://docs.gcp.databricks.com/en/dev-tools/bundles/deployment-modes.html)\n\n[Databricks on AWS](../../index.html)\n\nGet started\n\n  * [Get started](../../getting-started/index.html)\n  * [What is Databricks?](../../introduction/index.html)\n  * [DatabricksIQ](../../databricksiq/index.html)\n  * [Release notes](../../release-notes/index.html)\n\nLoad & manage data\n\n  * [Work with database objects](../../database-objects/index.html)\n  * [Connect to data sources](../../connect/index.html)\n  * [Connect to compute](../../compute/index.html)\n  * [Discover data](../../discover/index.html)\n  * [Query data](../../query/index.html)\n  * [Ingest data](../../ingestion/index.html)\n  * [Work with files](../../files/index.html)\n  * [Transform data](../../transform/index.html)\n  * [Schedule and orchestrate workflows](../../jobs/index.html)\n  * [Monitor data and AI assets](../../lakehouse-monitoring/index.html)\n  * [Share data securely](../../data-sharing/index.html)\n\nWork with data\n\n  * [Data engineering](../../data-engineering.html)\n  * [AI and machine learning](../../machine-learning/index.html)\n  * [Generative AI tutorial](../../generative-ai/tutorials/ai-cookbook/index.html)\n  * [Business intelligence](../../ai-bi/index.html)\n  * [Data warehousing](../../sql/index.html)\n  * [Notebooks](../../notebooks/index.html)\n  * [Delta Lake](../../delta/index.html)\n  * [Developers](../../languages/index.html)\n    * [Python](../../languages/python.html)\n    * [R](../../sparkr/index.html)\n    * [Scala](../../languages/scala.html)\n    * [SQL](../../sql/language-manual/index.html)\n    * [User-defined functions (UDFs)](../../udf/index.html)\n    * [Databricks Apps](../databricks-apps/index.html)\n    * [Tools](../index.html)\n      * [Authentication](../auth/index.html)\n      * [Databricks Connect](../databricks-connect/index.html)\n      * [Databricks extension for Visual Studio Code](../vscode-ext/index.html)\n      * [SDKs](../sdks.html)\n      * [SQL drivers and tools](../sql-drivers-tools.html)\n      * [Databricks CLI](../cli/index.html)\n      * [Databricks Asset Bundles](index.html)\n        * [Bundle development](work-tasks.html)\n        * [Bundle configuration file syntax](settings.html)\n          * Bundle deployment modes\n          * [Bundle run identity](run-as.html)\n          * [Resource permissions](permissions.html)\n          * [Cluster settings override](cluster-override.html)\n          * [Job task type settings](job-task-types.html)\n          * [Job task settings override](job-task-override.html)\n          * [Bundle library dependencies](library-dependencies.html)\n          * [Substitutions and variables](variables.html)\n          * [Dynamic artifacts settings](artifact-overrides.html)\n          * [Bundle configuration examples](resource-examples.html)\n        * [Bundles with jobs tutorial](jobs-tutorial.html)\n        * [Bundles with pipelines tutorial](pipelines-tutorial.html)\n        * [MLOps Stacks tutorial](mlops-stacks.html)\n        * [Python wheel builds tutorial](python-wheel.html)\n      * [CI/CD](../ci-cd.html)\n      * [Notebook utilities](../databricks-utils.html)\n  * [Technology partners](../../integrations/index.html)\n\nAdministration\n\n  * [Account and workspace administration](../../admin/index.html)\n  * [Security and compliance](../../security/index.html)\n  * [Data governance (Unity Catalog)](../../data-governance/index.html)\n  * [Lakehouse architecture](../../lakehouse-architecture/index.html)\n\nReference & resources\n\n  * [Reference](../../reference/api.html)\n  * [Resources](../../resources/index.html)\n  * [What’s coming?](../../whats-coming.html)\n  * [Documentation archive](../../archive/index.html)\n\nUpdated Nov 14, 2024\n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation\nFeedback)\n\n  * [Documentation](../../index.html)\n  * [Develop on Databricks](../../languages/index.html)\n  * [Developer tools](../index.html)\n  * [What are Databricks Asset Bundles?](index.html)\n  * [Databricks Asset Bundle configuration](settings.html)\n  * Databricks Asset Bundle deployment modes\n  * \n\n# Databricks Asset Bundle deployment modes\n\nAugust 23, 2024\n\nThis article describes the syntax for _Databricks Asset Bundle_ deployment\nmodes. Bundles enable programmatic management of Databricks workflows. See\n[What are Databricks Asset Bundles?](index.html)\n\nIn CI/CD workflows, developers typically code, test, deploy, and run solutions\nin various phases, or _modes_. For example, the simplest set of modes includes\na _development_ mode for pre-production validation, followed by a _production_\nmode for validated deliverables. Databricks Asset Bundles provides an optional\ncollection of default behaviors that correspond to each of these modes. To use\nthese behaviors for a specific target, set a `mode` or configure `presets` for\na target in the `targets` configuration mapping. For information on `targets`,\nsee [bundle configuration targets mapping](settings.html#targets).\n\n**In this article:**\n\n  * Development mode\n  * Production mode\n  * Custom presets\n\n## Development mode\n\nTo deploy your bundle in development mode, you must first add the `mode`\nmapping, set to `development`, to the intended target. For example, this\ntarget named `dev` is treated as a development target:\n\nCopy\n\nYAML\n\n    \n    \n    targets:\n      dev:\n        mode: development\n    \n\nDeploying a target in development mode by running the `databricks bundle\ndeploy -t <target-name>` command implements the following behaviors, which can\nbe customized using presets:\n\n  * Prepends all resources that are not deployed as files or notebooks with the prefix `[dev ${workspace.current_user.short_name}]` and tags each deployed job and pipeline with a `dev` Databricks tag.\n\n  * Marks all related deployed Delta Live Tables pipelines as `development: true`. See [Use development mode to run pipeline updates](../../delta-live-tables/testing.html#use-development-mode-to-run-pipeline-updates).\n\n  * Enables the use of `--compute-id <cluster-id>` in related calls to the `bundle deploy` command, which overrides any and all existing cluster definitions that are already specified in the related bundle configuration file. Instead of using `--compute-id <cluster-id>` in related calls to the `bundle deploy` command, you can set the `compute_id` mapping here, or as a child mapping of the `bundle` mapping, to the ID of the cluster to use.\n\n  * Pauses all schedules and triggers on deployed resources such as jobs or quality monitors. Unpause schedules and triggers for an individual job by setting `schedule.pause_status` to `UNPAUSED`.\n\n  * Enables concurrent runs on all deployed jobs for faster iteration. Disable concurrent runs for an individual job by setting `max_concurrent_runs` to `1`.\n\n  * Disables the deployment lock for faster iteration. This lock prevents deployment conflicts which are unlikely to occur in dev mode. Re-enable the lock by setting `bundle.deployment.lock.enabled` to `true`.\n\n## Production mode\n\nTo deploy your bundle in production mode, you must first add the `mode`\nmapping, set to `production`, to the intended target. For example, this target\nnamed `prod` is treated as a production target:\n\nCopy\n\nYAML\n\n    \n    \n    targets:\n      prod:\n        mode: production\n    \n\nDeploying a target in production mode by running the `databricks bundle deploy\n-t <target-name>` command implements the following behaviors:\n\n  * Validates that all related deployed Delta Live Tables pipelines are marked as `development: false`.\n\n  * Validates that the current Git branch is equal to the Git branch that is specified in the target. Specifying a Git branch in the target is optional and can be done with an additional `git` property as follows:\n\nCopy\n\nYAML\n\n    \n        git:\n      branch: main\n    \n\nThis validation can be overridden by specifying `--force` while deploying.\n\n  * Databricks recommends that you use service principals for production deployments. You can enforce this by setting `run_as` to a service principal. See [Manage service principals](../../admin/users-groups/service-principals.html) and [Specify a run identity for a Databricks Asset Bundles workflow](run-as.html). If you do not use service principals, then note the following additional behaviors:\n\n    * Validates that `artifact_path`, `file_path`, `root_path`, or `state_path` mappings are not overridden to a specific user.\n\n    * Validates that the `run_as` and `permissions` mappings are specified to clarify which identities have specific permissions for deployments.\n\n  * Unlike the preceding behavior for setting the `mode` mapping to `development`, setting the `mode` mapping to `production` does not allow overriding any existing cluster definitions that are specified in the related bundle configuration file, for instance by using the `--compute-id <cluster-id>` option or the `compute_id` mapping.\n\n## Custom presets\n\nDatabricks Asset Bundles supports configurable presets for\n[targets](settings.html#targets), which allows you to customize the behaviors\nfor targets. The available presets are listed in the following table:\n\nPreset | Description  \n---|---  \n`name_prefix` | The prefix string to prepend to resource names.  \n`pipelines_development` | Whether or not the pipeline is in development mode. Valid values are `true` or `false`.  \n`trigger_pause_status` | A pause status to apply to all triggers and schedules. Valid values are `PAUSED` or `UNPAUSED`.  \n`jobs_max_concurrent_runs` | The number of maximum allowed concurrent runs for jobs.  \n`tags` | A set of key:value tags that apply to all resources that support tags, which includes jobs and experiments. Databricks Asset Bundles do not support tags for the `schema` resource.  \n  \nNote\n\nIf both `mode` and `presets` are set, presets override the default mode\nbehavior, and settings of individual resources override the presets. For\nexample, if a schedule is set to `UNPAUSED`, but the `trigger_pause_status`\npreset is set to `PAUSED`, the schedule is unpaused.\n\nThe following example shows a custom presets configuration for the target\nnamed `dev`:\n\nCopy\n\nYAML\n\n    \n    \n    targets:\n      dev:\n        presets:\n          name_prefix: "testing_"      # prefix all resource names with testing_\n          pipelines_development: true  # set development to true for pipelines\n          trigger_pause_status: PAUSED # set pause_status to PAUSED for all triggers and schedules\n          jobs_max_concurrent_runs: 10 # set max_concurrent runs to 10 for all jobs\n          tags:\n            department: finance\n    \n\n* * *\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the\nSpark logo are trademarks of the [Apache Software\nFoundation](http://www.apache.org/).\n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback) | [Privacy Policy](https://databricks.com/privacy-policy) | [Terms of Use](https://databricks.com/terms-of-use)\n\n### In this article:\n\n  * Development mode\n  * Production mode\n  * Custom presets',
                outbound_links=[
                    ArtifactSummary(
                        artifact_id="07b5d56c-2a49-4ed9-86be-bf80947b7dce",
                        url="https://docs.databricks.com/en/delta-live-tables/testing.html",
                        title="Tips, recommendations, and features for developing and testing Delta Live Tables pipelines | Databricks on AWS",
                        summary="This article provides insights into best practices for developing and testing Delta Live Tables pipelines. It discusses configurations to isolate pipelines in various environments—development, testing, and production—using SQL and Python code. The recommendations aim to enhance code extensibility and resiliency testing, including the use of parameters and Git for effective pipeline management.",
                    ),
                    ArtifactSummary(
                        artifact_id="a728c41c-626c-45e5-a3ea-34cc2c366a82",
                        url="https://docs.databricks.com/en/dev-tools/bundles/settings.html",
                        title="Databricks Asset Bundle configuration | Databricks on AWS",
                        summary="This article provides a comprehensive guide on the syntax and structure required for Databricks Asset Bundle configuration files, essential for defining and managing Databricks Asset Bundles. It covers minimum requirements, including the mandatory `databricks.yml` file, and offers insights into working with YAML format. Additionally, it presents examples and mappings that illustrate how to effectively create and manage configuration files for various use cases in Databricks.",
                    ),
                    ArtifactSummary(
                        artifact_id="64031448-a781-4e61-aed4-cac3f90e58db",
                        url="https://docs.databricks.com/en/dev-tools/bundles/index.html",
                        title="What are Databricks Asset Bundles? | Databricks on AWS",
                        summary="Databricks Asset Bundles (DABs) are tools designed to implement software engineering best practices like source control, testing, and CI/CD for data and AI projects. They allow users to define Databricks resources as source files which streamline collaboration and deployment. Bundles encompass metadata, project structure, and testing configurations, ensuring a comprehensive development approach.",
                    ),
                    ArtifactSummary(
                        artifact_id="07faeabe-e08b-4ab4-a88a-ec75cdc0b991",
                        url="https://docs.databricks.com/en/dev-tools/bundles/run-as.html",
                        title="Specify a run identity for a Databricks Asset Bundles workflow | Databricks on AWS",
                        summary="This article describes how to use the `run_as` setting to specify the identity to use when running Databricks Asset Bundles workflows. The `run_as` setting allows for the separation of the identity used to deploy a bundle job from that used by the job or pipeline workflow, enhancing flexibility and security. It also outlines best practices for configuring run identities for different deployment environments.",
                    ),
                    ArtifactSummary(
                        artifact_id="8ca339e3-8a69-4855-a4fd-1e4b51aaace1",
                        url="https://docs.databricks.com/en/admin/users-groups/service-principals.html",
                        title="Manage service principals | Databricks on AWS",
                        summary="This article explains how to create and manage service principals for your Databricks account and workspaces. Service principals are identities created for use with automated tools, jobs, and applications, providing API-only access to Databricks resources. The document covers the identity model, access control, and detailed procedures for managing service principals at both account and workspace levels.",
                    ),
                ],
                inbound_links=[
                    ArtifactSummary(
                        artifact_id="132b5c17-a7d4-47cd-a218-e9cc82d31990",
                        url="https://docs.databricks.com/en/dev-tools/ci-cd.html",
                        title="What is CI/CD on Databricks? | Databricks on AWS",
                        summary="This article provides an introduction to Continuous Integration and Continuous Delivery (CI/CD) on Databricks, detailing its importance in software development and data engineering. By automating the building, testing, and deployment of code, teams can achieve more reliable releases compared to manual processes. It emphasizes the use of Databricks Asset Bundles for managing complex data and ML projects, facilitating easier deployment and management across various environments.",
                    )
                ],
            ),
            ArtifactWithLinks(
                artifact_id="22de1eb2-5314-4e95-bf4b-769eafb5e670",
                url="https://docs.databricks.com/en/dev-tools/databricks-apps/get-started.html",
                title="Get started with Databricks Apps | Databricks on AWS",
                summary="This article serves as a comprehensive guide to help users get started with Databricks Apps. It provides a step-by-step example for creating a simple app in a local development environment and deploying it to a Databricks workspace. Users will learn how to set up their environment, add source code, test their app locally, and deploy it to their workspace.",
                parsed_text="* __[](https://www.databricks.com/)\n\n  * __[](https://www.databricks.com/)\n  * [Help Center](https://help.databricks.com/s/)\n  * [Documentation](https://docs.databricks.com/en/index.html)\n  * [Knowledge Base](https://kb.databricks.com/)\n\n  * [Community](https://community.databricks.com)\n  * [Support](https://help.databricks.com)\n  * [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n  * [Try Databricks](https://databricks.com/try-databricks)\n\n[English](javascript:void\\(0\\))\n\n  * [日本語](../../../ja/dev-tools/databricks-apps/get-started.html)\n  * [Português](../../../pt/dev-tools/databricks-apps/get-started.html)\n\n[Amazon Web Services](javascript:void\\(0\\))\n\n  * [Microsoft Azure](https://learn.microsoft.com/en/azure/databricks/dev-tools/databricks-apps/get-started)\n  * [Google Cloud Platform](https://docs.gcp.databricks.com/en/dev-tools/databricks-apps/get-started.html)\n\n[Databricks on AWS](../../index.html)\n\nGet started\n\n  * [Get started](../../getting-started/index.html)\n  * [What is Databricks?](../../introduction/index.html)\n  * [DatabricksIQ](../../databricksiq/index.html)\n  * [Release notes](../../release-notes/index.html)\n\nLoad & manage data\n\n  * [Work with database objects](../../database-objects/index.html)\n  * [Connect to data sources](../../connect/index.html)\n  * [Connect to compute](../../compute/index.html)\n  * [Discover data](../../discover/index.html)\n  * [Query data](../../query/index.html)\n  * [Ingest data](../../ingestion/index.html)\n  * [Work with files](../../files/index.html)\n  * [Transform data](../../transform/index.html)\n  * [Schedule and orchestrate workflows](../../jobs/index.html)\n  * [Monitor data and AI assets](../../lakehouse-monitoring/index.html)\n  * [Share data securely](../../data-sharing/index.html)\n\nWork with data\n\n  * [Data engineering](../../data-engineering.html)\n  * [AI and machine learning](../../machine-learning/index.html)\n  * [Generative AI tutorial](../../generative-ai/tutorials/ai-cookbook/index.html)\n  * [Business intelligence](../../ai-bi/index.html)\n  * [Data warehousing](../../sql/index.html)\n  * [Notebooks](../../notebooks/index.html)\n  * [Delta Lake](../../delta/index.html)\n  * [Developers](../../languages/index.html)\n    * [Python](../../languages/python.html)\n    * [R](../../sparkr/index.html)\n    * [Scala](../../languages/scala.html)\n    * [SQL](../../sql/language-manual/index.html)\n    * [User-defined functions (UDFs)](../../udf/index.html)\n    * [Databricks Apps](index.html)\n      * Get started with apps\n      * [Develop apps](app-development.html)\n      * [Configure your app](configuration.html)\n    * [Tools](../index.html)\n  * [Technology partners](../../integrations/index.html)\n\nAdministration\n\n  * [Account and workspace administration](../../admin/index.html)\n  * [Security and compliance](../../security/index.html)\n  * [Data governance (Unity Catalog)](../../data-governance/index.html)\n  * [Lakehouse architecture](../../lakehouse-architecture/index.html)\n\nReference & resources\n\n  * [Reference](../../reference/api.html)\n  * [Resources](../../resources/index.html)\n  * [What’s coming?](../../whats-coming.html)\n  * [Documentation archive](../../archive/index.html)\n\nUpdated Nov 14, 2024\n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation\nFeedback)\n\n  * [Documentation](../../index.html)\n  * [Develop on Databricks](../../languages/index.html)\n  * [What is Databricks Apps?](index.html)\n  * Get started with Databricks Apps\n  * \n\n# Get started with Databricks Apps\n\nNovember 01, 2024\n\nPreview\n\nDatabricks Apps is in [Public Preview](../../release-notes/release-\ntypes.html).\n\nThis article helps you get started with Databricks Apps using a step-by-step\nexample to create a simple app in your local development environment and\ndeploy the app to your Databricks workspace. This example walks you through:\n\n  * Creating and testing the app locally.\n\n  * After testing locally, using the Databricks CLI to add the app to your Databricks workspace.\n\n  * Viewing the details page for the app in your workspace.\n\n  * Copying the source code and artifacts for the app to your workspace.\n\n  * Viewing the output of the app in your workspace.\n\nBefore stepping through the example, ensure that your Databricks workspace and\nlocal development environment meet the [requirements](index.html#app-reqs).\n\nDatabricks recommends using a Python virtual environment when developing apps.\nThe example in this article uses [pipenv](https://pipenv.pypa.io/en/latest/)\nto create a virtual environment. To learn more, see [Python Virtual\nEnvironments: A Primer](https://realpython.com/python-virtual-environments-a-\nprimer/).\n\nThis example is also available in the Databricks Apps template library. See\n[How do I create an app in the Databricks Apps UI?](app-\ndevelopment.html#create-apps-ui).\n\n**In this article:**\n\n  * Step 1: Set up your local environment\n  * Step 2: Add the source and configuration for your app\n  * Step 3: Test your app locally\n  * Step 4: Deploy the app to your workspace\n  * Next steps\n\n## Step 1: Set up your local environment\n\nOpen a terminal and run the following commands to:\n\n  * Create and start a Python virtual environment.\n\n  * Install the Python libraries required by the example app.\n\n  * Create a local directory for the source and configuration files for your app.\n\nCopy\n\nBash\n\n    \n    \n    pipenv --python 3.11\n    pipenv shell\n    pip install gradio\n    pip install pandas\n    mkdir <app-dir-name>\n    cd <app-dir-name>\n    \n\nReplace `<app-dir-name>` with the name of a local directory for your app\nfiles, for example, `gradio-hello-world`.\n\n## Step 2: Add the source and configuration for your app\n\n  1. In a text editor or your favorite integrated development environment (IDE), create a new Python file with the following code and save it to the directory you created. This example uses the filename `app.py` for the Python file:\n\nCopy\n\nPython\n\n    \n        import gradio as gr\n    import pandas as pd\n    \n    data = pd.DataFrame({'x': [x for x in range(30)],\n                         'y': [2 ** x for x in range(30)]})\n    \n    # Display the data with Gradio\n    with gr.Blocks(css='footer {visibility: hidden}') as gradio_app:\n        with gr.Row():\n            with gr.Column(scale=3):\n                gr.Markdown('# Hello world!')\n                gr.ScatterPlot(value=data, height=400, width=700,\n                               container=False, x='x', y='y',\n                               y_title='Fun with data', x_title='Apps')\n    \n    if __name__ == '__main__':\n        gradio_app.launch()\n    \n\n  2. In a text editor or an IDE, create a new YAML file with the following contents and save it to a file named `app.yaml` in the directory you created:\n\nCopy\n\nYAML\n\n    \n        command: [\n      \"python\",\n      \"<app-name.py>\"\n    ]\n    \n\nReplace `<app-name.py>` with the name of the Python file containing the code\nfor the app. For example, `app.py`.\n\n## Step 3: Test your app locally\n\n  1. To test your app locally, open a terminal and run `python <app-name.py>`, replacing `<app-name.py>` with the name of the file containing the code for the app.\n\nCopy\n\nBash\n\n    \n        python app.py\n    Running on local URL:  http://127.0.0.1:7860\n    ...\n    \n\n  2. To view the app’s output, open `http://127.0.0.1:7860` in a browser window.\n\n## Step 4: Deploy the app to your workspace\n\nTo create a new app in your workspace and deploy the code from your local\nenvironment to the workspace, open a terminal and complete the following\nsteps.\n\n  1. Create the app in your Databricks workspace.\n\nNote\n\n     * The name assigned to a Databricks app cannot be changed after creating the app, and any user with access to a Databricks workspace can see the names and deployment history of all Databricks apps in the workspace. Additionally, the app name is included in records written to system tables. Because of this visibility, you should not include sensitive information when naming your Databricks apps.\n\n     * The name must be unique in the Databricks workspace that hosts the app and must contain only lowercase letters, numbers, and hyphens.\n\nCopy\n\nBash\n\n    \n        databricks apps create <app-name>\n    \n\nReplace `<app-name>` with a name for your app. For example, `gradio-hello-\nworld`.\n\n  2. To view the app in your workspace when the `create` command completes, in the sidebar, click  **Compute** , go to the **Apps** tab, and click the link to your app in the **Name** column.\n\n  3. Sync the files from your local environment to your Databricks workspace. The command to sync the files from your local environment to your workspace, including the workspace path for the files, is under **Sync source files into Databricks**. Click  to copy this command.\n\n  4. In a terminal, switch to the directory containing your app files and run the copied `sync` command.\n\nNote\n\nIf there are specific files or directories in your local app directory that\nyou do not want transferred by the `databricks sync` command, add those files\nor directories to a `.gitignore` file in the local app directory. For example,\nif you have a Python virtual environment directory in the same directory as\nyour app, add the directory’s name to the `.gitignore` file, and the `sync`\ncommand will skip that directory when transferring files.\n\nCopy\n\nBash\n\n    \n        databricks sync --watch . /Workspace/Users/user@databricks.com/gradio-hello-world\n    ...\n    Initial Sync Complete\n    \n\n  5. To view the synced files in your workspace when the `sync` command completes, click  **Workspace** in the sidebar and go to the directory created for your app.\n\n  6. To deploy the app, run the following command in a terminal, replacing `<app-path>` with the workspace path to your app files.\n\n`databricks apps deploy gradio-hello-world --source-code-path <app-path>`\n\n  7. To view the deployment status, go to the details page for the app.\n\nTo view the deployed app’s output, click the app link under the app name on\nthe details page.\n\n## Next steps\n\nTo learn how to create apps in the Databricks Apps UI, see [How do I create an\napp in the Databricks Apps UI?](app-development.html#create-apps-ui).\n\n* * *\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the\nSpark logo are trademarks of the [Apache Software\nFoundation](http://www.apache.org/).\n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback) | [Privacy Policy](https://databricks.com/privacy-policy) | [Terms of Use](https://databricks.com/terms-of-use)\n\n### In this article:\n\n  * Step 1: Set up your local environment\n  * Step 2: Add the source and configuration for your app\n  * Step 3: Test your app locally\n  * Step 4: Deploy the app to your workspace\n  * Next steps",
                outbound_links=[
                    ArtifactSummary(
                        artifact_id="b378acd7-87af-4936-9da9-8708fd86936d",
                        url="https://docs.databricks.com/en/dev-tools/databricks-apps/index.html",
                        title="What is Databricks Apps? | Databricks on AWS",
                        summary="Databricks Apps allows developers to create secure data and AI applications on the Databricks platform, enabling easy sharing with users. This feature simplifies the application development process by hosting apps on Databricks, eliminating the need for separate infrastructure and compliance management. Developers can utilize Python frameworks and leverage Databricks resources like Unity Catalog and Databricks SQL for their applications.",
                    ),
                    ArtifactSummary(
                        artifact_id="0e29b542-a743-4208-9eef-8425ae65926a",
                        url="https://docs.databricks.com/en/dev-tools/databricks-apps/app-development.html",
                        title="Develop Databricks Apps | Databricks on AWS",
                        summary="This article provides comprehensive guidance on creating data and AI applications using Databricks Apps. It covers how to create and edit apps in the Databricks UI, utilize essential platform features like SQL warehouses and Databricks Jobs, and offers best practices for development. Additionally, it addresses configuration, permissions, logging, and the use of secrets for sensitive information, ensuring developers can effectively implement and manage their applications.",
                    ),
                ],
                inbound_links=[
                    ArtifactSummary(
                        artifact_id="b378acd7-87af-4936-9da9-8708fd86936d",
                        url="https://docs.databricks.com/en/dev-tools/databricks-apps/index.html",
                        title="What is Databricks Apps? | Databricks on AWS",
                        summary="Databricks Apps allows developers to create secure data and AI applications on the Databricks platform, enabling easy sharing with users. This feature simplifies the application development process by hosting apps on Databricks, eliminating the need for separate infrastructure and compliance management. Developers can utilize Python frameworks and leverage Databricks resources like Unity Catalog and Databricks SQL for their applications.",
                    )
                ],
            ),
        ]
    },
}

AFTER_PLANNING: ContextVariables = {
    **AFTER_TOPIC_RESEARCH_COMPLETE,
    "runbook_sections": [
        RunbookSection(
            section_title="Step 1: Set up Databricks Account and AWS Integration",
            outline="- Goals: Set up the foundational integration between Databricks and AWS\n - High level steps: 1. Create and configure Databricks account 2. Integrate Databricks with AWS 3. Configure permissions and roles",
            related_artifacts=[
                "263b9b55-023b-4315-867b-3d393cfd7707",
                "ee044e97-b3e0-40de-a4a5-9098e21c5cc0",
            ],
            content=None,
        ),
        RunbookSection(
            section_title="Step 2: Launch Databricks Workspace on AWS",
            outline="- Goals: Deploy and access Databricks workspace on AWS\n - High level steps: 1. Deploy the Databricks workspace 2. Access the workspace URL and tools 3. Familiarize with core Databricks services and UI",
            related_artifacts=["263b9b55-023b-4315-867b-3d393cfd7707"],
            content=None,
        ),
        RunbookSection(
            section_title="Step 3: Configure Data and Compute Storage",
            outline="- Goals: Establish data storage and compute configurations\n - High level steps: 1. Connect to AWS S3 for data storage 2. Set up compute resources within AWS for Databricks",
            related_artifacts=["e4d26a2c-fcba-4387-9e22-eff18b18a14b"],
            content=None,
        ),
        RunbookSection(
            section_title="Step 4: Test Deployment with Basic ETL Workflow",
            outline="- Goals: Validate the basic setup with a simple ETL task\n - High level steps: 1. Create a sample ETL task using Databricks 2. Execute the task and validate outputs",
            related_artifacts=["263b9b55-023b-4315-867b-3d393cfd7707"],
            content=None,
        ),
        RunbookSection(
            section_title="Step 5: Optional Advanced Configurations",
            outline="- Goals: Explore further configurations and enhancements if required\n - High level steps: 1. Investigate Databricks on AWS integrations 2. Explore asset bundle deployment modes",
            related_artifacts=[
                "ee044e97-b3e0-40de-a4a5-9098e21c5cc0",
                "22de1eb2-5314-4e95-bf4b-769eafb5e670",
            ],
            content=None,
        ),
    ],
    "current_runbook_section": 0,
}
